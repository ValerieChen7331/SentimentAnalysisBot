import json
import re
from llm_api import LLMAPI

class LLMHelper:
    def __init__(self):
        self.llm_option = "Gemma2:27b"
        self.mode = "å…§éƒ¨LLM"

    def analyze_article(self, article: str) -> dict:
        prompt = (
            "è«‹æ ¹æ“šä»¥ä¸‹æ–°èå…§å®¹ï¼ŒåŒæ™‚ç”¢ç”Ÿï¼š\n"
            "1. æ–°èæ‘˜è¦ï¼ˆ200 å­—å…§ï¼‰\n"
            "2. æƒ…ç·’åˆ¤æ–·ï¼ˆæ­£é¢ã€ä¸­æ€§ã€è² é¢æ“‡ä¸€ï¼‰\n"
            "3. å‘½åå¯¦é«”ï¼ˆåŒ…å«äººåã€å…¬å¸ã€åœ°åï¼Œä»¥é€—è™Ÿåˆ†éš”ï¼‰\n"
            "âš ï¸ æœ€å¾Œè«‹å‹™å¿…ä»¥ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š\n"
            '{ "summary": "...", "sentiment": "...", "ner": "..." }\n\n'
            "ä»¥ä¸‹ç‚ºæ–°èå…§å®¹ï¼š\n" + article
        )

        llm = LLMAPI().get_llm(self.mode, self.llm_option)
        response_text = llm.invoke(prompt)
        print("ğŸ§  LLM å›å‚³åŸå§‹çµæœï¼š", response_text)

        try:
            result = json.loads(response_text)
        except json.JSONDecodeError:
            cleaned_text = response_text.replace("\n", " ").replace("\r", "").strip()
            match = re.search(r'\{.*\}', cleaned_text)
            if match:
                try:
                    result = json.loads(match.group(0))
                except:
                    result = {"summary": "ç„¡æ³•ç”Ÿæˆæ‘˜è¦", "sentiment": "æœªçŸ¥", "ner": "ç„¡"}
            else:
                result = {"summary": "ç„¡æ³•ç”Ÿæˆæ‘˜è¦", "sentiment": "æœªçŸ¥", "ner": "ç„¡"}
        return result

    def generate_summary(self, articles: list, analyses: list) -> str:
        combined_text = []
        for i, (article, analysis) in enumerate(zip(articles, analyses), start=1):
            block = (
                f"ã€ç¬¬ {i} å‰‡æ–°èã€‘\n"
                f"æ¨™é¡Œï¼š{article['title']}\n"
                f"æ—¥æœŸï¼š{article['publish_date']}\n"
                f"æ‘˜è¦ï¼š{analysis['summary']}\n"
                f"æƒ…ç·’ï¼š{analysis['sentiment']}\n"
                f"NERï¼š{analysis['ner']}\n"
            )
            combined_text.append(block)

        prompt = (
            "ä»¥ä¸‹ç‚ºå¤šç¯‡æ–°èå…§å®¹åŠ AI åˆ†æçµæœï¼Œè«‹ç¶œåˆä»¥ä¸‹å…§å®¹ç”¢å‡º 300 å­—ä»¥å…§çš„è¼¿æƒ…ç¸½çµï¼Œ"
            "ä¸¦èªªæ˜è¼¿è«–è¶¨å‹¢åŠæ½›åœ¨é¢¨éšªï¼š\n\n" + "\n\n".join(combined_text)
        )

        llm = LLMAPI().get_llm(self.mode, self.llm_option)
        response_text = llm.invoke(prompt)

        if response_text.startswith("æŸ¥è©¢å¤±æ•—"):
            return "âš ï¸ ç„¡æ³•ç”Ÿæˆæ‘˜è¦ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

        return response_text

    def query_to_keywords(self, query: str) -> str:
        import streamlit as st
        prompt = f"""
        è«‹æ ¹æ“šä»¥ä¸‹ä½¿ç”¨è€…å•é¡Œï¼Œç”¢ç”Ÿ 1-3 å€‹æœ€é©åˆæ–°èæœå°‹çš„çŸ­é—œéµå­—ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰ï¼Œ
        é—œéµå­—æ‡‰ç°¡æ½”ä¸”ä»£è¡¨ä¸»é¡Œé‡é»ã€‚
        è«‹ä»¥æ­¤æ ¼å¼å›æ‡‰ï¼š
        <qtkeywords> é—œéµå­—1, é—œéµå­—2, é—œéµå­—3 <qtkeywords>

        ä½¿ç”¨è€…è¼¸å…¥: {query}
        """

        llm = LLMAPI().get_llm(self.mode, self.llm_option)
        response_text = llm.invoke(prompt)
        print(f"ğŸ” LLM é—œéµå­—å›æ‡‰: {response_text}")
        match = re.search(r"<qtkeywords>\s*(.*?)\s*<qtkeywords>", response_text)
        if match:
            keywords_str = match.group(1)
            keywords_list = [kw.strip() for kw in re.split(r"[ï¼Œ,]", keywords_str)]
            print(f"âœ… é—œéµå­—æ¸…å–®: {keywords_list}")
            return keywords_list[0]
        else:
            # Fallbackï¼šè‹¥ç„¡æ³•åŒ¹é…æ ¼å¼ï¼Œå˜—è©¦ç”¨å›å‚³æ–‡å­—ç¬¬ä¸€è¡Œæˆ–å‰å¹¾å€‹å­—ä½œç‚ºé—œéµå­—
            fallback_keyword = response_text.strip().split('\n')[0][:10]
            print(f"âš ï¸ ä½¿ç”¨ fallback keyword: {fallback_keyword}")
            st.warning(f"âš ï¸ LLM å›å‚³åŸå§‹çµæœï¼š{response_text}")
            return fallback_keyword or ""

