import streamlit as st
from news_database import NewsDatabase
from llm_helper import LLMHelper
from bnext_news_crawler import BnextNewsCrawler
import datetime

# å¢åŠ è¨˜æ†¶åŠŸèƒ½ï¼Œè¨˜å¾—è³‡æ–™
# æ‡‰åƒ…å›ç­”èˆ‡è¼¿æƒ…åˆ†æç›¸é—œçš„å•é¡Œï¼Œå°æ–¼ç„¡é—œå•é¡Œæ‡‰é©ç•¶æ‹’ç­”ã€‚

# åˆå§‹åŒ– SQLite è³‡æ–™åº«
db = NewsDatabase()

# æ¨™é¡Œ
st.title("ğŸ“¡ æ–°èè¼¿æƒ…åˆ†æå¹³å°")

# å»ºç«‹åˆ†é 
tab1, tab2 = st.tabs(["ğŸ” æœå°‹æ–°è", "ğŸ“‚ å·²å„²å­˜æ–°è"])

# === Tab1: è‡ªå‹•æœå°‹ + è‡ªå‹•åˆ†æ ===
with tab1:
    st.header("ğŸ” å¾ã€æ•¸ä½æ™‚ä»£ã€è‡ªå‹•æœå°‹ä¸¦åˆ†æ")
    crawler = BnextNewsCrawler(headless=True)

    # ä½¿ç”¨ form æ”¯æ´ Enter è§¸ç™¼
    with st.form(key="search_form"):
        query = st.text_input("è«‹è¼¸å…¥æ–°èä¸»é¡Œæˆ–å•é¡Œ...").strip()
        submit_button = st.form_submit_button("æœå°‹")  # æŒ‰ Enter æˆ–æŒ‰æ­¤æŒ‰éˆ•éƒ½å¯ä»¥è§¸ç™¼

    if submit_button:
        # é€é LLM æŠ½å–æœ€é©æœå°‹é—œéµå­—
        keyword = LLMHelper.query_to_keywords(query)
        if not keyword:
            st.warning("âš ï¸ LLM ç„¡æ³•è§£æå‡ºæœ‰æ•ˆé—œéµå­—ï¼Œè«‹æ›å€‹æè¿°è©¦è©¦ã€‚")
        else:
            with st.spinner(f"ğŸ” æ­£åœ¨æœå°‹ã€Œ{keyword}ã€æ–°è..."):
                search_results = crawler.search_news(keyword, max_results=3)

            if not search_results:
                st.error("â— æ²’æœ‰æ‰¾åˆ°æ–°èï¼Œè«‹æ›å…¶ä»–é—œéµå­—ã€‚")
            else:
                all_details = []

                # âœ… ç¬¬ 1 æ­¥ï¼šå…ˆæ‰¹æ¬¡æŠ“å–è©³ç´°æ–‡ç« å…§å®¹
                with st.spinner(f"ğŸ” æ­£åœ¨çˆ¬èŸ²ã€Œ{keyword}ã€æ–°è..."):
                    for idx, article in enumerate(search_results, start=1):
                        detail = crawler.fetch_article_content(article['url'])
                        all_details.append(detail)
                # âœ… çˆ¬èŸ²å®Œæˆå¾Œæ‰æç¤ºã€Œé–‹å§‹åˆ†æã€
                st.success(f"âœ… å·²æˆåŠŸæŠ“å– {len(all_details)} ç¯‡æ–°è")

                all_analyses = []

                # âœ… ç¬¬ 2 æ­¥ï¼šé€ç¯‡é€²è¡Œ LLM åˆ†æ
                for idx, detail in enumerate(all_details, start=1):
                    with st.spinner(f"ğŸ§  åˆ†æç¬¬ {idx} ç¯‡æ–°èä¸­..."):
                        analysis = LLMHelper.analyze_article(detail['content'])
                        crawler.save_to_db(detail, analysis)  # åŒæ­¥å„²å­˜è‡³è³‡æ–™åº«
                        all_analyses.append(analysis)

                    # é¡¯ç¤ºæ¯ç¯‡æ–°èåˆ†æçµæœ
                    with st.expander(f"ã€ç¬¬ {idx} ç¯‡ã€‘ {detail['publish_date']} - {detail['title']}"):
                        st.write(f"ğŸ”— [å‰å¾€åŸæ–‡]({detail['url']})")
                        st.write(detail['content'][:200] + "...")
                        st.write("**ğŸ“„ LLM æ‘˜è¦**:", analysis["summary"])
                        st.write("**ğŸ˜Š æƒ…ç·’åˆ¤æ–·**:", analysis["sentiment"])
                        st.write("**ğŸ·ï¸ å‘½åå¯¦é«”**:", analysis["ner"])

                # âœ… ç¬¬ 3 æ­¥ï¼šç¶œåˆå¤šç¯‡æ–°èç”¢å‡ºè¼¿æƒ…ç¸½çµ
                with st.spinner("ğŸ§  æ­£åœ¨ç”Ÿæˆç¶œåˆè¼¿æƒ…æ‘˜è¦..."):
                    all_news_summary = LLMHelper.generate_summary(all_details, all_analyses)
                    st.subheader("ğŸ“¢ ç¶œåˆè¼¿æƒ…æ‘˜è¦")
                    st.write(all_news_summary)

# === Tab2: æŸ¥çœ‹å·²å„²å­˜æ–°è ===
with tab2:
    st.header("ğŸ“‚ ç€è¦½å·²å„²å­˜æ–°è")
    keyword_filter = st.text_input("ğŸ” å¯è¼¸å…¥é—œéµå­—ç¯©é¸ï¼ˆç•™ç©ºé¡¯ç¤ºå…¨éƒ¨ï¼‰").strip()
    today = datetime.date.today()
    past_30_days = today - datetime.timedelta(days=30)

    saved_news = db.search_news(
        keyword=keyword_filter,
        date_from=str(past_30_days),
        date_to=str(today)
    )

    if saved_news:
        st.info(f"ğŸ“š æœ€è¿‘ 30 å¤©å…± {len(saved_news)} ç¯‡å·²å„²å­˜æ–°èï¼š")
        for idx, record in enumerate(saved_news, start=1):
            with st.expander(f"{record[1]} - {record[0]}"):
                st.write(record[2][:300] + "...")
                st.write(f"ğŸ‘‰ [å‰å¾€åŸæ–‡é€£çµ]({record[3]})")
    else:
        st.info("âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°èè¨˜éŒ„ã€‚")
