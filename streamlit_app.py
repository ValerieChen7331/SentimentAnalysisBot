# streamlit_app.py
# æä¾›æ–°èè‡ªå‹•æœå°‹ã€LLM åˆ†æã€å„²å­˜åŠæ­·å²ç´€éŒ„æŸ¥è©¢åŠŸèƒ½
import streamlit as st
import datetime
from apis.news_database import NewsDatabase
from models.llm_helper import LLMHelper
from models.crawler_bnext import BnextNewsCrawler

# åˆå§‹åŒ–æœ¬åœ° SQLite è³‡æ–™åº«
db = NewsDatabase()
# é¡¯ç¤º Streamlit é¦–é æ¨™é¡Œ
st.title("ğŸ“¡ æ–°èè¼¿æƒ…åˆ†æå¹³å°")
# å»ºç«‹åˆ†é ä»‹é¢ï¼ˆåˆ†æˆæœå°‹èˆ‡æ­·å²ï¼‰
tab_search, tab_history = st.tabs(["ğŸ” æœå°‹æ–°è", "ğŸ“‚ å·²å„²å­˜æ–°è"])

def process_single_article(idx: int, article_detail: dict, llm_helper, crawler) -> dict:
    """
    åˆ†æå–®ç¯‡æ–°èï¼Œä¸¦æ–¼ä»‹é¢ä¸­é¡¯ç¤ºçµæœï¼ŒåŒæ™‚å„²å­˜åˆ°è³‡æ–™åº«ã€‚
    """
    analysis = llm_helper.analyze_article(article_detail['content'])
    crawler.save_to_db(article_detail, analysis)  # å„²å­˜çµæœåˆ°è³‡æ–™åº«

    # å°‡çµæœå±•é–‹åœ¨ç•«é¢ä¸Š
    with st.expander(f"ã€ç¬¬ {idx} ç¯‡ã€‘ {article_detail['publish_date']} - {article_detail['title']}"):
        st.write(f"ğŸ”— [å‰å¾€åŸæ–‡]({article_detail['url']})")
        st.write(article_detail['content'][:200] + "...")
        st.write("**ğŸ“„ LLM æ‘˜è¦**:", analysis["summary"])
        st.write("**ğŸ˜Š æƒ…ç·’åˆ¤æ–·**:", analysis["sentiment"])
        st.write("**ğŸ·ï¸ å‘½åå¯¦é«”**:", analysis["ner"])
    return analysis


def handle_search():
    """
    1. è™•ç†ä½¿ç”¨è€…è¼¸å…¥ï¼Œç¢ºèªæ˜¯å¦é©åˆè¼¿æƒ…åˆ†æï¼Œ
    2. å°‡å•é¡Œé€é LLM è½‰æˆæœå°‹é—œéµå­—ã€çˆ¬å–æ–°èã€åˆ†æä¸¦ç¶œåˆçµæœã€‚
    """
    st.header("ğŸ” å¾ã€æ•¸ä½æ™‚ä»£ã€è‡ªå‹•æœå°‹ä¸¦åˆ†æ")
    crawler = BnextNewsCrawler(headless=True)  # åˆå§‹åŒ–æ–°èçˆ¬èŸ²
    llm_helper = LLMHelper()  # åˆå§‹åŒ– LLM è¼”åŠ©å·¥å…·

    # è¼¸å…¥æ¬„ä½
    with st.form(key="search_form"):
        user_query = st.text_input("è«‹è¼¸å…¥æ–°èä¸»é¡Œæˆ–å•é¡Œ...").strip()
        start_search = st.form_submit_button("æœå°‹")

    # ä½¿ç”¨è€…é»åŠæœå°‹
    if start_search:
        # é©—è­‰è¼¸å…¥å•é¡Œæ˜¯å¦é©åˆè¼¿æƒ…åˆ†æ
        with st.spinner(f"ğŸ” é©—è­‰ä¸­..."):
            is_search = llm_helper.is_semantic_search(user_query)
        if not is_search:
            st.warning("âš ï¸ æ­¤å•é¡Œä¸é©åˆè¼¿æƒ…åˆ†æï¼Œè«‹æ›å€‹æè¿°")
            return

        # ä½¿ç”¨ LLM å°‡å•é¡Œè½‰æ›æˆæœå°‹ç”¨çš„é—œéµå­—
        with st.spinner(f"ğŸ” æ­£åœ¨è§£æå‡ºæœ‰æ•ˆé—œéµå­—..."):
            keyword = llm_helper.query_to_keywords(user_query)
        if not keyword:
            st.warning("âš ï¸ LLM ç„¡æ³•è§£æå‡ºæœ‰æ•ˆé—œéµå­—ï¼Œè«‹æ›å€‹æè¿°å†è©¦")
            return

        # ä½¿ç”¨é—œéµå­—é–‹å§‹æ–°èæœå°‹
        with st.spinner(f"ğŸ” æ­£åœ¨æœå°‹ã€Œ{keyword}ã€æ–°è..."):
            articles = crawler.search_news(keyword, max_results=3)
        if not articles:
            st.error("â— æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–°èï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—")
            return

        # æ“·å–æ¯ç¯‡æ–°èè©³ç´°å…§å®¹
        with st.spinner(f"ğŸ” æŠ“å–ç›¸é—œæ–°è..."):
            article_details = [crawler.fetch_article_content(article['url']) for article in articles]
        st.success(f"âœ… æˆåŠŸæŠ“å– {len(article_details)} ç¯‡æ–°è")

        # å°æ¯ç¯‡æ–°èé€²è¡Œåˆ†æèˆ‡ç•«é¢å±•ç¤º
        analyses = []
        for idx, article_detail in enumerate(article_details, start=1):
            with st.spinner(f"ğŸ§  æ­£åœ¨åˆ†æç¬¬ {idx} ç¯‡æ–°è..."):
                analysis = process_single_article(idx, article_detail, llm_helper, crawler)
                analyses.append(analysis)

        # ç¶œåˆç”¢å‡ºè¼¿æƒ…æ‘˜è¦
        with st.spinner("ğŸ§  æ­£åœ¨ç¶œåˆç”¢å‡ºè¼¿æƒ…æ‘˜è¦..."):
            summary = llm_helper.generate_summary(article_details, analyses)
            st.subheader("ğŸ“¢ ç¶œåˆè¼¿æƒ…æ‘˜è¦")
            st.write(summary)


def show_history():
    """
    é¡¯ç¤ºæœ€è¿‘ 30 å¤©å…§å·²å„²å­˜æ–°èè¨˜éŒ„ï¼Œä¸¦å¯é€éé—œéµå­—ç¯©é¸ã€‚
    """
    st.header("ğŸ“‚ ç€è¦½å·²å„²å­˜æ–°è")
    keyword_filter = st.text_input("ğŸ” å¯è¼¸å…¥é—œéµå­—ç¯©é¸ï¼ˆç•™ç©ºå‰‡é¡¯ç¤ºå…¨éƒ¨ï¼‰").strip()
    today = datetime.date.today()
    past_30_days = today - datetime.timedelta(days=30)

    # å¾è³‡æ–™åº«ä¸­æŸ¥è©¢æ–°è
    saved_news = db.search_news(keyword=keyword_filter, date_from=str(past_30_days), date_to=str(today))

    if not saved_news:
        st.info("âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°èè¨˜éŒ„")
        return

    st.info(f"ğŸ“š æœ€è¿‘ 30 å¤©å…§å…± {len(saved_news)} ç¯‡æ–°èï¼š")
    for title, date, content, url in saved_news:
        with st.expander(f"{date} - {title}"):
            st.write(content[:300] + "...")
            st.write(f"ğŸ‘‰ [å‰å¾€åŸæ–‡]({url})")


# Streamlit åŸ·è¡Œä¸»æµç¨‹
if __name__ == "__main__":
    with tab_search:
        handle_search()

    with tab_history:
        show_history()
