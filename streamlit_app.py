import streamlit as st
from news_database import NewsDatabase
from llm_helper import LLMHelper
from bnext_news_crawler import BnextNewsCrawler
import datetime

# åˆå§‹åŒ– SQLite è³‡æ–™åº«
db = NewsDatabase()

# æ¨™é¡Œ
st.title("ğŸ“¡ æ–°èè¼¿æƒ…åˆ†æå¹³å°")

# å»ºç«‹åˆ†é 
tab1, tab2 = st.tabs(["ğŸ” æœå°‹æ–°è", "ğŸ“‚ å·²å„²å­˜æ–°è"])

# === Tab1: è‡ªå‹•æœå°‹ + è‡ªå‹•åˆ†æ ===
with tab1:
    st.header("ğŸ” å¾ã€æ•¸ä½æ™‚ä»£ã€è‡ªå‹•æœå°‹ä¸¦åˆ†æ")
    crawler = BnextNewsCrawler(headless=True)
    llm_helper = LLMHelper()  # âœ… å„ªåŒ–ï¼šåªå»ºç«‹ä¸€æ¬¡ LLMHelper å¯¦ä¾‹

    with st.form(key="search_form"):
        query = st.text_input("è«‹è¼¸å…¥æ–°èä¸»é¡Œæˆ–å•é¡Œ...").strip()
        submit_button = st.form_submit_button("æœå°‹")

    if submit_button:
        keyword = llm_helper.query_to_keywords(query)
        if not keyword:
            st.warning("âš ï¸ LLM ç„¡æ³•è§£æå‡ºæœ‰æ•ˆé—œéµå­—ï¼Œè«‹æ›å€‹æè¿°è©¦è©¦ã€‚")
        else:
            with st.spinner(f"ğŸ” æ­£åœ¨æœå°‹ã€Œ{keyword}ã€æ–°è..."):
                search_results = crawler.search_news(keyword, max_results=3)

            if not search_results:
                st.error("â— æ²’æœ‰æ‰¾åˆ°æ–°èï¼Œè«‹æ›å…¶ä»–é—œéµå­—ã€‚")
            else:
                all_details = []

                with st.spinner(f"ğŸ” æ­£åœ¨çˆ¬èŸ²ã€Œ{keyword}ã€æ–°è..."):
                    for idx, article in enumerate(search_results, start=1):
                        detail = crawler.fetch_article_content(article['url'])
                        all_details.append(detail)

                st.success(f"âœ… å·²æˆåŠŸæŠ“å– {len(all_details)} ç¯‡æ–°è")

                all_analyses = []

                for idx, detail in enumerate(all_details, start=1):
                    with st.spinner(f"ğŸ§  åˆ†æç¬¬ {idx} ç¯‡æ–°èä¸­..."):
                        analysis = llm_helper.analyze_article(detail['content'])  # âœ… æ”¹ç”¨å…±ç”¨ç‰©ä»¶
                        crawler.save_to_db(detail, analysis)
                        all_analyses.append(analysis)

                    with st.expander(f"ã€ç¬¬ {idx} ç¯‡ã€‘ {detail['publish_date']} - {detail['title']}"):
                        st.write(f"ğŸ”— [å‰å¾€åŸæ–‡]({detail['url']})")
                        st.write(detail['content'][:200] + "...")
                        st.write("**ğŸ“„ LLM æ‘˜è¦**:", analysis["summary"])
                        st.write("**ğŸ˜Š æƒ…ç·’åˆ¤æ–·**:", analysis["sentiment"])
                        st.write("**ğŸ·ï¸ å‘½åå¯¦é«”**:", analysis["ner"])

                with st.spinner("ğŸ§  æ­£åœ¨ç”Ÿæˆç¶œåˆè¼¿æƒ…æ‘˜è¦..."):
                    all_news_summary = llm_helper.generate_summary(all_details, all_analyses)  # âœ… æ”¹ç”¨å…±ç”¨ç‰©ä»¶
                    st.subheader("ğŸ“¢ ç¶œåˆè¼¿æƒ…æ‘˜è¦")
                    st.write(all_news_summary)

# === Tab2: æŸ¥çœ‹å·²å„²å­˜æ–°è ===
with tab2:
    st.header("ğŸ“‚ ç€è¦½å·²å„²å­˜æ–°è")
    keyword_filter = st.text_input("ğŸ” å¯è¼¸å…¥é—œéµå­—ç¯©é¸ï¼ˆç•™ç©ºé¡¯ç¤ºå…¨éƒ¨ï¼‰").strip()
    today = datetime.date.today()
    past_30_days = today - datetime.timedelta(days=30)

    saved_news = db.search_news(
        keyword=keyword_filter,
        date_from=str(past_30_days),
        date_to=str(today)
    )

    if saved_news:
        st.info(f"ğŸ“š æœ€è¿‘ 30 å¤©å…± {len(saved_news)} ç¯‡å·²å„²å­˜æ–°èï¼š")
        for idx, record in enumerate(saved_news, start=1):
            with st.expander(f"{record[1]} - {record[0]}"):
                st.write(record[2][:300] + "...")
                st.write(f"ğŸ‘‰ [å‰å¾€åŸæ–‡é€£çµ]({record[3]})")
    else:
        st.info("âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°èè¨˜éŒ„ã€‚")
